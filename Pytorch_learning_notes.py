# -*- coding: utf-8 -*-
"""
Created on Tue Aug 13 13:25:45 2019

@author: lisong
"""

import torch
import torchvision
import torch.nn as nn
#nn是工具箱，里面包含必须是所有net的超类Module模块，以及经常使用的Functional模块。我们在定义网络的时候
#必须这样
"""
class net(nn.Module):
    def __init__(self):
        super(net,self).__init__()
        
        ....
#构造函数必须继承自nn.Module
"""
import torch.nn.functional as F

卷积神经网络的结构是怎样的？包含哪些要素
卷积神经网络就是说可以通过一个过滤器fliter，对原样本做出的一个卷积，filter是一个矩阵，和原数据矩阵相乘
得到一个实数，相当于提取出了一个特征，选择合适大小的filter，就可以实现原矩阵的再变换。这个fliter其实就是
输入层之下的一个隐层，不止是一个隐层是很多隐层，具体的结构我一时半会想不出来。但是可以想象filter的矩阵的
元素就是需要学习的权重，被卷积之后的矩阵才是隐层矩阵。所以Relu激活函数就很好理解了，如果超过阈值了。
就保持输入值不变，否则为0.

在卷积时如何提取特征涉及到stride步长这个参数，这个参数对应着初次卷积隐层的数量，所以这和隐层数目是一个超
参数不谋而合，需要人来调试。

padding参数用来调整是否应该让第一次卷积之后产生的隐层神经元数量和原样本输入神经元数量相同。若相同，
称为卷积之后大小不变，称为'same'方式，否则称为'valid'方式。

池层.pooling layer。池化技术是为了提取一个区域内的一些重要特征，是为了减少特征数量，降低过拟合风险的做法。
从参数的字面意义就是说有提取一个区域内的max或average，比较简单粗暴的降维手段。

全连接层，fully-connected layer。就是和前馈神经网络类似的结构，因为此时经过多轮压缩，理论上说应该已经
完成了必要的特征提取，这样我们为了最后的输出更有效率，使用了传统的神经网络来对输出做调整，增加隐层和
隐层神经元的数目，提升数据的解释能力。而全连接层的数量也是超参数，是需要我们进行调整的。设置2个全连接层
也是为了不同的激活函数对其做调整。这已经是集成学习的概念了。

卷积网络的好处：
1.参数共享机制，因为图片每个像素作为一个特征，那么初始特征会很多，稍微增加一些隐层，需要学习的权重数就会
呈指数性增加，但是filter之前已经分析过，实际上就是权重，在提取图片特征的时候，filter是不变的，因此需要学习
的权重就被定义为这个filter的大小。一方面减少权重的数量，另一方面降低过拟合风险。第三是fliter不变的话，
矩阵特征因为平移不变性，哪怕图片进行了平移操作，也可以识别出特征。

2.连接的稀疏性，因为只使用了原图片的部分特征，所以不需要那么多的连接，（同上述权重的减少），而且因为
特征识别的特性，该区域的特征不会被其他区域所影响。

以上摘自某位知乎大神，学习吴恩达课程后的笔记。深入浅出。至少搞清楚了超参数有哪些，

按照一般模型构造实验的步骤，一般分为环境搭建，网络定义，数据加载和处理，训练模型和检验模型。
那么官方的Tutorial是怎么组织的呢？这是今天的功课。
tensor.view 返回一个tensor的视图，包含原tensor的所有数据但可能有不同的shape，可以为其提供不同的shape
参数，但可以传入一个-1，让其根据另一个维度来推断。不仅包含2维，n维tensor也是可以的。
tensor和Numpy的ndarray是类似的，也可以通过tensor.numpy()方法将其从tensor转为ndarray。而且此时两个变量
共享同一块内存区域，修改一方会使得另一方也修改。
tensor操作方法比如add，加入_可以使得原地修改，比如x.add_(y)会使得直接在x上修改。这个设定很人性化。

在网络定义中，构造函数主要定义有哪些层layer，比如卷积层有几个卷积层的输入层的维度，输出层的维度
核的大小，跨度多大等。卷积层到连接层之后，连接层有几层，每一层的接受输入维度和输出维度。这个类的
最主要方法就是net.forward()这个方法应该是Module中有的，但同时用来规定，数据是如何在这些层里加工的。
比如每个层的激活函数是什么，全连接层的激活函数是什么，因为神经网络重要的就是更新连接权重，激活函数的
阈值也是一种权重，所以构造函数定义了有多少权重需要更新，forward()函数则定义了数据如何通过隐层，但因为
autograd的存在，那么我们只需要定义forward，而无需定义backward。他会自动反向传播，但是如何获得当时的梯度
还没看明白。

torch.nn在接收数据时，只接收mini-batch且非单个样本。mini-batch的意思就是把整个数据集打散后，化整为零，
以此通过模型训练并更新，因为每次只使用了数据集的一小部分，使得loss更新不如整体计算更好。
所以很可能会导致学习器的性能有所下降，但因为通常训练不止一轮，加上使用的梯度下降的更新模式，
相当于实现了样本扰动下的随机梯度下降，是一种集成学习的思想。因此在保证了模型损失在可接受范围内，
大部分普通人的机器都能运转，这不失为一个好的选择。

在定义好网络和激活函数后，现在需要定义损失函数。loss function有很多，都放在nn里，我们可以定义不同的
损失函数。损失函数由模型的输出和数据标记来计算，并输出一个定值。这个定值是tensor类，并根据你定义的损失
函数绑定一个grad_fn属性。有了损失之后，需要调整梯度。按照逆向传播的顺序，逐一获取每个变量的偏导值。
要逆向传播仅需要调用loss.backward()即可。只要调用一次，就会按照逆向传播的顺序，更新原变量的偏导值。

取得了各变量偏导值，则需要考量的就是如何更新权重。最直接的就是SGD，随机梯度下降stochastic gradient decent
定义怎样的学习率，怎样的更新公式，这个也留给了使用者。可以调用nn.parameters获得参数集，并逐一根据
当前学习率和梯度更新。

如何更新。一个模块叫做torch.optim优化模块来控制，里面提供了成熟的优化方法等我们调用具体先实例化
一个optimizer，然后调用optimizer.step()

torch.utils.data.Dataset 是一个抽象类，这个类需要overwrite __getitem__和__len__两个内置方法。
前者在抽象类中只有一个占位符类型的方法，作用是按照索引的方式来获取Dataset中的数据，len方法是确定数据
集的size，这两个作为接口会被其他类所使用。
获得了Dataset子类，就可以使用DataLoader类来加载处理data。这个含义是DataLoader提供了很多参数比如
shuffle，batchsize，之类的，而DataLoader返回的是一个类似于迭代器的实例，这样可以根据shuffle和
batchsize来提取数据用于训练时，节省内存。这样方便对数据的预处理。

model.eval()等价于model.train(False)。也就是说在做模型测试时，需要关闭训练模式，否则输入的testdataset
会被当做训练数据来更新权值。

DataLoader创建时除了可以用重写了getitem和len两个方法的类以外，主要是为了使用map方法。还有一种是
iterabledataset的子类。需要重写__iter__方法，这样是方便对于一些体量非常大的数据集，不适合shuffle的。
如果使用多线程加载数据，原数据集会被复制给每一个进程，那么需要特殊的设置，才能避免获得重复的数据。
iterabledataset是如何迭代的，取决于用户定义，甚至可以定义动态的batch size，或者chunk。

DataLoader的参数，sampler就是一个采集器，用于采集一篮子索引，batch_size用于初始化batch_sampler
告诉这一篮子索引应该如何切分，同时drop_last用于不足batch_size的时候是否截断，collate_fn用于在
获得了索引后如何整理。这些都是可以自定义的类。只要其具有相应行为的接口即可。

autograd是为某一个张量的属性。为什么是具体数值呢，因为是这个点的导数值，所以其维度是和该张量的维度一致的。
用二维打比方，grad保存的[x,y]处的值。只有通过计算图产生的tensor，才有grad_fn属性，自己创设的叫做leaf节点。
会有自动更新的梯度值，但是不会有grad_fn。为包含一个元素的标量的tensor求backward，不需要指定形状。否则
需要匹配一个特定的参数形状。

因为有autograd这个在，所以在计算图生成的过程中，一组数据可能会被反复使用,最关键是梯度只会累计，不会自动
删除。所以在load batch的时候，以往的梯度需要先删除，才能和当前batch size相匹配。至于例子中，那只是
规范操作而已。

因为计算图的关系，当你使用loss函数求取Loss的时候，实际上也被计算图纳入进去了，而这些都是根源于你建立的
叶节点，leaf node，这样，即使使用loss函数求出的标量tensor，可以直接调用backward，甚至不用指定匹配的
形状。backward不算更新，只是自动计算，这是一个开关。你点下去，梯度开始计算，但你还需要更新他。所以
再使用optim模块对这个参数集集中处理。这个参数集保存在nn.parameters中。更新完毕，开始执行下一步。
就是optim.step()

tensor返回的shape，这样理解，从最高维向低维。也就是说a.shape=[1,2,3,4].表示1个样本有2个矩阵，每个矩阵都是
3行4列。行和列的关系还是行在前，列在后。4维tensor用别人的话就是说代表了一个batch，第一个数字的含义就是batch_size。


卷积过滤后输出矩阵的大小为((n-f+2*P)/s)+1。这个公式里n是输入矩阵的大小，f是核的大小，P是padding的大小
s是stride的大小。但在pytorch里面定义卷积层，有输入通道，输出通道，padding，stride，kenerl_size。
基本上就是这个公式里涵盖了。通道可以看成这个平面矩阵的数量。按照道理来说，训练数据的维度是
(n,c,h,w)也就是n数据数量，c通道数，h长度，w宽度。卷积层其实就是权重，之前已经了解。初始化很重要。

池化层的输出大小公式和卷积层一样，为何呢?卷积层也是计算矩阵乘法。